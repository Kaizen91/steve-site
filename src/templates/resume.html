{% extends "base.html" %}
{% block content %}
<div class="container">
<h2 id="stephen-lang-resume">Stephen Lang Resume</h2>
<p><strong>Email</strong>: <a href="mailto:stephenlang91@gmail.com"
class="email">stephenlang91@gmail.com</a> <strong>Website</strong>: <a
href="https://stevelang.xyz" class="uri">https://stevelang.xyz</a></p>
<p><strong>GitHub</strong>: <a href="https://github.com/Kaizen91"
class="uri">https://github.com/Kaizen91</a> <strong>LinkedIn</strong>:
<a href="https://www.linkedin.com/in/stephen-lang-canada/"
class="uri">https://www.linkedin.com/in/stephen-lang-canada/</a></p>
<table>
<colgroup>
<col style="width: 22%" />
<col style="width: 25%" />
<col style="width: 21%" />
<col style="width: 31%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Software Development</strong></th>
<th><strong>Data Engineering Tools</strong></th>
<th><strong>DevOps Tools</strong></th>
<th><strong>Soft Skills</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>-SQL</td>
<td>-ETL / ELT</td>
<td>-Google Cloud Platform</td>
<td>-Spanish / English</td>
</tr>
<tr class="even">
<td>-Python</td>
<td>-OLAP / OLTP</td>
<td>-Terraform</td>
<td>-Project Management (Kanban, Agile)</td>
</tr>
<tr class="odd">
<td>-R</td>
<td>-Spark / Hadoop</td>
<td>-Linux</td>
<td>-Ability to train both technical and non technical audiences</td>
</tr>
<tr class="even">
<td>-Continuous Integration / Continuous Deployment</td>
<td>-BigQuery, Cloud Storage, Pub/Sub, Dataproc</td>
<td>-Cloud Computing generally</td>
<td></td>
</tr>
<tr class="odd">
<td>-Git</td>
<td>-DBT</td>
<td>-Docker</td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="work-experience">Work Experience</h2>
<p><strong>Data Engineer Manager</strong> Scotiabank 11.2020 -
10.2023</p>
<p>I was the team lead for a team of data engineers building ETL
pipelines to produce standardized data sets on a project spanning 4
countries. We worked using Pyspark to build an ETL pipeline to
standardize the bank’s data. I handled several Data Quality and
remediation initiatives. I provided training to several teams on GCP and
cloud computing in general. I handled Data Governance activities for
dozens of systems.</p>
<p><strong>Professional Services Consultant</strong> Delbridge Solutions
03.2019 - 11.2020</p>
<p>Built ETL processes using SQL stored procedures, Bash Scripts, APIs,
and mapping tools.<br />
Consolidations, Capex planning, HR planning, OPEX, COGS, and Revenue
Planning<br />
Implementing and integrating VENA Financial Budgeting Software.<br />
Training power users on OLAP technologies.<br />
Designing reporting and budgeting workflows, and designing Solution
Architecture Addressing security and data permissions.<br />
Wrote Python scripts to automate repetitive processes and increase
efficiency<br />
Leveraging data base connections and Excel to build custom Business
Intelligence reports</p>
<p><strong>Application Specialist</strong> Ceridian 06.2017 -
03.2019</p>
<p>Complex problem solving related to both payroll law and data
analysis<br />
Running SQL queries and scripts to investigate and solve issues<br />
Developing BI reporting<br />
Bridge between the client and product management team ensuring any
product enhancements are built into software<br />
Solving technical issues related to processing payroll for multimillion
dollar clients<br />
Creating training documentation<br />
Configuring taxation for clients with employees in multiple states</p>
<h2 id="certifications">Certifications</h2>
<ul>
<li>Google Cloud Certified Professional Cloud Architect</li>
<li>Google Cloud Certified Associate Cloud Engineer</li>
</ul>
<h2 id="projects">Projects</h2>
<p><strong>Canadian Housing Market Pipeline</strong></p>
<p><a href="https://github.com/Kaizen91/spark-housing-market-canada"
class="uri">https://github.com/Kaizen91/spark-housing-market-canada</a></p>
<p>An ETL project using canadian housing data to demonstrate knowledge
of Spark, Terraform, and GCP (Dataproc, Cloud Storage, BigQuery). The
main.tf terraform file will create all the infrastructure needed for
this pipeline: a Google Cloud Storage bucket, a Dataproc cluster, a
Dataproc job, and a BigQuery dataset. It will also upload the source csv
file and the transform.py script to the Google Cloud Storage bucket, so
that they can be accessed by the Dataproc Pyspark Job running on the
Dataproc cluster.</p>
<p><strong>GCP Streaming Pipeline with Cloud Run</strong></p>
<p><a href="https://github.com/Kaizen91/gcp-stream-cloud-run"
class="uri">https://github.com/Kaizen91/gcp-stream-cloud-run</a></p>
<p>This is an ETL project using GCP (Pub/Sub, Docker, Cloud Run, and
BigQuery) to stream simulated telemetry data. This set up would be good
for any pipeline where you need to do light weight transformations on
messages before storing them.</p>
<h2 id="education">Education</h2>
<h4 id="carleton-university-2011---2015">Carleton University 2011 -
2015</h4>
<ul>
<li>Bachelor’s Degree, International Business, Finance and Banking
Concentration Graduated with Honors</li>
</ul>
</div>
{% endblock content %}
